{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some important admin and homework for next time.\n",
    "\n",
    "\n",
    "1. __Important point number 1__. We are reaching the mid-term of the semester and it is time for some reflection :) I want to know from you guys how the class is going. This is super important for me to make sure that the second half of the semester goes well. Before working on the material for today, please __fill [the mid-term survery ](https://docs.google.com/forms/d/e/1FAIpQLSc79NAmXoPfYjmVyVgCDUdyK8narAOuR-CmgYhQM6h3vRZfuA/viewform?usp=sf_link)__ on Google Forms. I promise it should not take more than 5 minutes. \n",
    "\n",
    "\n",
    "2. __Important point number 2__. We are slowly approaching the time when you guys will work on your own project. There are still a couple of classes to go, but by now you have a good idea of the methods and types of questions we work with in this class. And I would like to start discussing your ideas. Make sure you __complete Exercise 5 (at the end of this notebook) before next Wednesday__. We will take some time to talk about it next time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of today's class.\n",
    "\n",
    "This week's curriculum is about text analysis. The overview is\n",
    "\n",
    "* Tricks for raw text (NLPP Chapter 3) and finding the important words in a document (TF-IDF)\n",
    "* Apply these tricks to study the content of submissions \n",
    "\n",
    "In the first part, we will take a quick tour of NLPP's chapter 3, which is boring, but an amazing resource that you'll keep returning to. Then we'll talk about how we can use simple statistics to get text to show us what it's all about. We will even do a little visualization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Processing real text (from out on the inter-webs)\n",
    "\n",
    "Ok. So Chapter 3 in NLPP is all about working with text from the real world. Getting text from this internet, cleaning it, tokenizing, modifying (e.g. stemming, converting to lower case, etc) to get the text in shape to work with the NLTK tools you've already learned about - and many more. In the process we'll learn more about regular expressions, as well as unicode; something we've already been struggling with a little bit will now be explained in more detail. \n",
    "> \n",
    "> **Video lecture**: Short overview of chapter 3 + a few words about kinds of language processing that we don't address in this class. \n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"Rwakh-HXPJk\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Reading*: NLPP Chapter 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.9, and 3.10\\. It's not important that you go in depth with everything here - the key thing is that you *know that Chapter 3 of this book exists*, and that it's a great place to return to if you're ever in need of an explanation of regular expressions, unicode, or other topics that you forget as soon as you stop using them (and don't worry, I forget about those things too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise_ 1: Just a couple of examples from the book: Work through the exercises NLPP 3.12: 6, 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NLPP 3.12: 6:\n",
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "    [a-zA-Z]+\n",
    "    [A-Z][a-z]*\n",
    "    p[aeiou]{,2}t\n",
    "    \\d+(\\.\\d+)?\n",
    "    ([^aeiou][aeiou][^aeiou])*\n",
    "    \\w+|[^\\w\\s]+\n",
    "\n",
    "Test your answers using nltk.re_show().\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Operator \tBehavior\n",
    ". \tWildcard, matches any character\n",
    "^abc \tMatches some pattern abc at the start of a string\n",
    "abc$ \tMatches some pattern abc at the end of a string\n",
    "[abc] \tMatches one of a set of characters\n",
    "[A-Z0-9] \tMatches one of a range of characters\n",
    "ed|ing|s \tMatches one of the specified strings (disjunction)\n",
    "* \tZero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
    "+ \tOne or more of previous item, e.g. a+, [a-z]+\n",
    "? \tZero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
    "{n} \tExactly n repeats where n is a non-negative integer\n",
    "{n,} \tAt least n repeats\n",
    "{,n} \tNo more than n repeats\n",
    "{m,n} \tAt least m and no more than n repeats\n",
    "a(b|c)+ \tParentheses that indicate the scope of the operators\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"[a-zA-Z]+: A string that contains at least one letter, either lower or upper case.\")\n",
    "print(\"[A-Z][a-z]*: A string with zero or more letters in the alphabet.\")\n",
    "print(\"p[aeiou]{,2}t: \")\n",
    "\n",
    "nltk.re_show(\"[a-zA-Z]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\nthough), 'I won't have any pepper in my kitchen AT ALL. Soup does very\nwell without--Maybe it's always pepper that makes people hot-tempered,'...\n'when i'm a duchess,' she said to herself, (not in a veri hope tone\nthough), 'I won't have ani pepper in my kitchen AT all. soup doe very\nwel without--mayb it' alway pepper that make peopl hot-tempered,'...\n'when i'm a duchess,' she said to herself, (not in a very hop tone\nthough), 'i won't hav any pep in my kitch at all. soup doe very\nwell without--maybe it's alway pep that mak peopl hot-tempered,'...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NLPP 3.12: 30:\n",
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences.\"\"\"\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "tokens = [\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n",
    "'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n",
    "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe',\n",
    "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
    "\n",
    "print(' '.join(tokens))\n",
    "\n",
    "tokens_porter = [porter.stem(w) for w in tokens]\n",
    "tokens_lancaster = [lancaster.stem(w) for w in tokens]\n",
    "\n",
    "print(' '.join(tokens_porter))\n",
    "print(' '.join(tokens_lancaster))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude to part 2. -  Data.\n",
    "\n",
    "In the follwing exercises, we will study the text contained in _r/wallstreebets_ submissions. To make things a bit more exciting, we will work with \\**all** the submissions posted in 2020 in _r/wallstreebets_. As you may well guess, we will need both the title and the content of each submission.\n",
    "\n",
    "To make things a bit less tedious for you guys, I downloaded and made avaialble the data you need (you can find it [here](https://github.com/lalessan/comsocsci2021/blob/master/data/wallstreet_subs.csv.gz)). The dataset consists of all the submissions posted between January 1st and December 31st 2020 with content in English. For each submission, you have the following information: timestamp of creation (__created_utc__), __title__, textual content (__selftext__), and __score__. You are welcome to use this data. If you prefer to download your own (see optional exercise below), that's even better!! As usual, I do not expect you to find a perfect match between your data and mine. In the exercises below, I refer to this data as the \"_wallstreetbets submissions dataset_\".\n",
    "\n",
    "_Exercise (Optional)_: \n",
    "\n",
    "> * Download all submissions posted on _r/wallstreetbets_ in 2020 using [psaw](https://pypi.org/project/psaw/).\n",
    "> * For each submission, keep the title, the textual content, the score, and the time of creation. \n",
    "> * Remove submissions whose content has been removed, and those that are not in English. You can use the library [langdetect](https://pypi.org/project/langdetect/) to detect the language of a given text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Words that characterize stocks discussed on r/wallstreetbets\n",
    "\n",
    "In this section, we'll begin to play around with how far we can get with simple strategies for looking at text. The video is Sune talking about a fun paper, which shows you how little is needed in order to reveal something very interesting about humans that produce text. Then, we'll use a very simple weighting scheme called TF-IDF to find the words in the reddit r/wallstreetbets submissions that charachterize different stocks. In cleaning the Reddit submissions, we'll use some of the stuff you've just read about above. Finally, we'll even visualize them in a fun little word cloud (below is what I found for the discussions around Gamestop, Microsoft, and Tesla). The wordclouds may not be immediately understandable. But if you do some research on the important words, you will find that the TF-IDF method extracts quite interesting information.\n",
    "\n",
    "<img src=\"https://github.com/lalessan/comsocsci2021/blob/master/files/wordclouds.png?raw=true\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Video lecture**: Simple methods reveal a lot. Sune talks a little bit about the paper: [Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073791).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"wkYvdfkVmlI\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 2: Most discussed stocks in r/wallstreebets_. GME is only one among many stocks people have discussed in _r/wallstreetbets_. In this exercise, we will find the most discussed stocks on _wallstreetbets_. Stocks are identified by their [Ticker symbol](https://en.wikipedia.org/wiki/Ticker_symbol). A Ticker symbol is nothing but a string consisting of letters and numbers, and is typically quite short. For example the Gamestop symbol is _GME_, Amazon is _AMZN_, Alphabet is _GOOGL_...\n",
    "\n",
    "> 1. To talk about a specific stock, Redditors often use the corresponding ticker symbol [preceded by the dollar sign](https://www.reddit.com/r/wallstreetbets/comments/5yvvue/why_do_you_put_a_dollar_sign_in_front_of_a_ticker/) (\\\\$GME, \\$AMZN...). Write down a [Regular Expression](https://en.wikipedia.org/wiki/Regular_expression) matching words that begin with a dollar sign \"\\\\$\". See [NLPP book, section 3.4]().\n",
    "> 2. Load the _wallstreetbets submission dataset_ as a Pandas DataFrame and create a new column containing both the title and the textual content of each submission (as one long string). We refer to this as the _text_ of the submission.\n",
    "> 3. For each submission, find all ticker symbols (those preceded by a dollar sign) contained in the _text_. Use the function [re.findall](https://docs.python.org/3/library/re.html), and the regular expression you created in point 1). Some tips for success: \n",
    "> > * Remove matches that are definetly not stock symbols (for example amounts like: \\\\$100, \\$1000k).\n",
    "> > * Convert all matches to uppercase\n",
    "> > * Remove the dollar sign at the beginning of the symbol (e.g. \\\\$gme → GME).\n",
    "> 4. Create a list containing the top 15 Ticker Symbols by number of occurrences. GME should be among them. If it is not, check again your analysis and/or come talk to me. Google the top 15 symbols and find the corresponding company names. Are they known companies or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "GME_data = pd.read_csv('/home/augustsemrau/drive/6semester/CSS_02467/comsocsci2021/lectures/data/week6/wallstreet_subs.csv', parse_dates=['created_utc']).set_index('created_utc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1. To talk about a specific stock, Redditors often use the corresponding ticker symbol [preceded by the dollar sign](https://www.reddit.com/r/wallstreetbets/comments/5yvvue/why_do_you_put_a_dollar_sign_in_front_of_a_ticker/) (\\\\$GME, \\$AMZN...). Write down a [Regular Expression](https://en.wikipedia.org/wiki/Regular_expression) matching words that begin with a dollar sign \"\\\\$\". See [NLPP book, section 3.4]().\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 3: TF-IDF and the stocks discussed on r/wallstreetbets._ The goal for this exercise is to find the words charachterizing each of the stocks discussed on r/wallstreetbets. We will focus on the top 15 stocks we idenfied in Exercise 2, and we will of course use TF-IDF.\n",
    "\n",
    " \n",
    "> 1. First, check out [the wikipedia page for TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Explain in your own words the point of TF-IDF. \n",
    ">   * What does TF stand for? \n",
    ">   * What does IDF stand for?\n",
    ">\n",
    "> 2. Tokenize the __text__ of each submission. Create a column __tokens__ in your dataframe containing the tokens. Remember the bullets below for success.\n",
    ">   * If you dont' know what _tokenization_ means, go back and read Chapter 3 again. **The advice to go back and check Chapter 3 is valid for every cleaning step below**.\n",
    ">   * Exclude punctuation.\n",
    ">   * Exclude URLs\n",
    ">   * Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    ">   * Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    ">   * Set everything to lower case.\n",
    ">   * **Note** that none of the above has to be perfect. And there's some room for improvisation. You can try using stemming. In my own first run the results didn't look so nice, because some submissions repeat certain words again and again and again, whereas other are very short. For that reason, I decided to use the unique set of words from each submission rather than each word in proportion to how it's actually used. Choices like that are up to you.\n",
    ">\n",
    "> 3. Find submissions discussing at least one of the top 15 stocks you identified above. To do so: \n",
    "> > * Create a function that finds the intersection between a list of tokens and your list of top 15 stocks. For example, your function applied to the tokens: _\"[Here, TSLA, submission, GME]\"_ should return [\"TSLA\",\"GME\"]. (_Optional_: you can also try to included cases in which the list of tokens contains a company name among your top 15. For example the function applied to _\"[Here, Gamestop, submission]\"_ could return ['GME'].)\n",
    "> > * Create a new column _stock_ in your DataFrame, containing the output of your function applied to the _text_ column. Values in this column should be lists. \n",
    "> > * Handle cases where one post discusses more than one stock by applying the function [__explode__](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) to the _stock_ column. This will duplicate submissions associated to multiple stocks. After exploding, the values included in the _stock_ column should be strings. \n",
    "> > * Handle cases where none of the selected stocks is discussed by replacing Nan values, for example with \"Other\".\n",
    ">\n",
    "> 4. Now, we want to find out which words are important for each *stock*, so we're going to create several ***large documents, one for each stock***. Each document includes all the tokens related to the same stock. We will also have a document including discussions that do not relate to the top 15 stocks.\n",
    "> 5. Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within __5 stocks of your choice__. \n",
    ">   * Describe similarities and differences between the stocks.\n",
    ">   * Why aren't the TFs not necessarily a good description of the stocks?\n",
    ">   * Next, we calculate IDF for every word. \n",
    ">   * What base logarithm did you use? Is that important?\n",
    "> 6. We're ready to calculate TF-IDF. Do that for the __5 stock of your choice__. \n",
    ">   * List the 10 top TF words for each stock.\n",
    ">  * List the 10 top TF-IDF words for each stock.\n",
    ">   * Are these 10 words more descriptive of the stock? If yes, what is it about IDF that makes the words more informative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _Exercise 4: The Wordcloud_. It's time to visualize our results!\n",
    "\n",
    "> * Install the [`WordCloud`](https://pypi.org/project/wordcloud/) module. \n",
    "> * Now, create word-cloud for each stock. Feel free to make it as fancy or non-fancy as you like. \n",
    "> * Comment on the results. Are these words to be expected? Is there anything that is surprising? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 5: A Study Project in Computational Social Science._\n",
    "> 1. Read the [Project Assignment](https://github.com/lalessan/comsocsci2021/wiki/Project-Assignment) page, where I explain how to set up a Study Project.\n",
    "> 2. Think of a topic of interest to your would like to study using data downloaded from the Web (Wikipedia, Twitter, Reddit, Facebook, Github, other data sources...), and some of the methods we have seen in this course. \n",
    "> 3. What is the topic? \n",
    "> 4. What is the data? \n",
    "> 5. Write down 3 research questions related to your topic that you would like to investigate.\n",
    "> 6. Put together 1 slide including the answers to points 3,4,5.\n",
    "\n",
    "__Important__: This will be by no means the final choice for your Project Assignment. All I want is for you guys to start thinking about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}